<!DOCTYPE html>
<html lang="en">

<head>

<meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>

  <title>Computer vision Project 1</title>
  <style>
    body {
      position: relative;

    }

    #table_of_content {
      width: 200px;
      position: fixed;
      font-size: 15px;
      height: 100%;
      overflow: auto;
    }

    #table_of_content a:hover {
      color: #000000;
    }

    #table_of_content a {
      padding: 6px 8px 6px 6px;
      text-decoration: none;
      color: #818181;
      display: block;
    }

    div #page-content-wrapper {
      margin-left: 192px;
      font-size: 15px;
      margin-top: 50px;
      padding-top: 6px;
      position: relative;
      height: 1000px;

    }





    #section1 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section2 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section3 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section4 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section5 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section6 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section7 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section8 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section9 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    h4 {
      text-align: center;
    }

    p.center {
      text-align: center;
    }
  </style>

</head>

<body data-spy="scroll" data-target="#table" data-offset="20">

  <!-- main navbar -->
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid">
      <div class="navbar-header">
        <a class="navbar-brand" href="#">Indoor Navigation Using Computer Vision</a>
      </div>
      <div>

        <ul class="nav navbar-nav">
          <!--      <li><a href="#">About</a></li> -->

        </ul>
      </div>
    </div>

  </nav>

  <!-- end of main navbar -->


  <div class="d-flex" id="wrapper">

    <!-- Sidebar -->
    <div class="bg-light border-right" id="table_of_content">


      <div class="list-group list-group-flush" id="table">

        <h4>Table of content</h4>
        <a href="#section1" class="list-group-item list-group-item-action bg-light">Introduction and Problem Definition</a>
        <a href="#section2" class="list-group-item list-group-item-action bg-light">Sensor Possibilities</a>
        <a href="#section3" class="list-group-item list-group-item-action bg-light">Indoor Navigation techniques</a>
        <a href="#section4" class="list-group-item list-group-item-action bg-light">Success and Failures</a>
        <a href="#section5" class="list-group-item list-group-item-action bg-light">Challenges</a>
        <a href="#section6" class="list-group-item list-group-item-action bg-light">Future </a>
        <a href="#section7" class="list-group-item list-group-item-action bg-light">Related Videos </a>
        <a href="#section8" class="list-group-item list-group-item-action bg-light">Quiz </a>
        <a href="#section9" class="list-group-item list-group-item-action bg-light">References </a>


      </div>
    </div>
    <!-- /#sidebar-wrapper -->

    <!-- Page Content -->
    <div id="page-content-wrapper">


      <div class="container-fluid p-0">
        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section1">
          <div class="w-100">

            <h1 class="mb-0">Introduction & Problem Definition

            </h1>

            <p> Many a time, I find it difficult to get direction indoors in a new place. I have seen many freshers at school having a hard time finding direction to their classrooms on the first day. It might get even harder for a person with low
              vision.
              <br>
              People are relying more on their smartphones for navigation.GPS works well for outdoor navigation, but it isn't accurate for indoor navigation. Currently, there is no indoor navigation system that is both economical and accurate.<br>
              The focus of this project is to find the current location of a user at indoors which will help in developing a solution to indoor navigation problem to help especially people who have low vision to navigate thru indoors.</p>

          </div>

        </section>

        <hr>


        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section2">
          <div class="w-100">
            <h1 class="mb-0">Sensor Possibilities

            </h1>

            <p>
              In this project,we will be using only the camera and sensors that are available in smartphones <br>
              There have been indoor navigation solutions using Infrared , Bluetooth beacons, wifi signal strength, RFID sensors, inertial sensors to track the user’s movements. These methods are not very economical as incur initial setup and
              maintenance cost. Also it is not practical to setup base staitions for every indoors that we need to navigate thru.

            </p>

          </div>
          <hr>
        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section3">
          <div class="w-100">
            <h1 class="mb-0">Indoor Navigation techniques based on computer vision

            </h1>
            <br>

            <h3>1.Vanishing Point Calculation</h3>
            <p>
              In this method, vanishing points are calculated from lines in consecutive images.Line features from images are selected and the vanishing points are calculated by choosing the pixel getting the maximum intersection points of line
              pairs.Some of the algorithms used in this techniques are:<br>



              <b> Gaussian filter: </b>This image processing techniques is used to reduce noise in the captured images. Once the noise reduction is done, the images will be sent for edge detection.
              <br>
              <b> Canny edge detection: </b> Edges are detected by the intensity gradient of pixels in the image. Pixels that have gradient magnitude above certain threshold are considered to make an edge.
              <br>
              <b> Hough transforms: </b>Lines that are edges has to be distinguished from the rest of the lines in the image. Hough line transform, and circle transform are used to detect lines and circles in the captured images respectively</p>
            <img src="images/corridor1.jpg" alt="corridor1" class="mx-auto d-block">
            <img src="images/corridor2.jpg" alt="corridor2" class="mx-auto d-block">

            <p>Fig <a href="#ref1">[1] </a>Image of corridor1 on top and corridor 2 on bottom. Blue lines were found with Hough transformation and the red spot shows the place of the vanishing point </p>


            <h3>2.VSLAM</h3>
            <p>
              Visual simultaneous localization and mapping (VSLAM) technique is used for building a 3D global map of unknown environment while simultaneously keeping track of current location.

              SLAM based systems consists of 4 parts:<br>
              <b>1.Sensor Data: </b>For VSLAM,data is obtained mainly from camera <br>
              <b>2. Front-end: </b>Feature points are unique location in images-example: Corners,T junctions.A good feature point is one that is reliable,i.e the algorithm must find the point in different viewing conditions(different camera
              angle,onrotation,lighting change,blurring from motion etc).SLAM algorithm also provides the 3D position of the 2D feature points that the system has been tracking. This set of 3D points is known as a “point cloud”.These features also
              need to be associated with landmarks – keypoints with a 3D position, also called map points. In addition, map points need to be tracked in a video stream.The back end can provide feedback to the front end for loop closure detection and
              verification.
              Long-term association reduces drift by recognizing places that have been encountered before (loop closure).<br>
              <b>3.Back-End:</b> Here, relationship between the video frames,device orientation and geometrical reconstruction is done.<br>
              <b>4.SLAM estimate: </b>This is the reslut from the above part that contains detected features and device postion in the area.<br><br>
              <img src="images/slamOverview.jpg" alt="slam overview" class="mx-auto d-block"><br>
              <p>Fig <a href="#ref2">[2] </a> SLAM based System overview</p><br>


              <img src="images/slam2.jpg" alt="slam system" class="mx-auto d-block"><br>
              <p>Fig <a href="#ref3">[3] </a>SLAM based System architecture</p><br>


              <img src="images/slammethods.jpg" alt="slam methods" class="mx-auto d-block"><br>
              <p>Fig <a href="#ref4">[4] </a> SLAM Methods</p>

              VSLAM is based on developing visual odometry from RGB-D data from the images.<a href="#ref5">[5] </a> Hence it requires per-pixel depth sensing RGB-D camera which isn’t available in most of the smartphones today. Since depth cannot be
              directly inferred from a
              single image from a smartphone camera, it must be calculated through analyzing images in the chain of frames in the video using an Extended Kalman filter. Also, this method has complex computations due to which there could be
              considerable amount of delay in 3D reconstructions. Hence it is difficult to use VSLAM technique for Indoor navigation solution using current day smartphones.
            </p>

            <h3>3.Computer Vision + Deep learnings</h3>
            <p>
              <br>
              In this technique, deep learning and computer vision algorithms are integrated.Smartphone camera is used to detect immovable objects like doors and windows in indoors to identify the
              current location.</p>
            <img src="images/systemflow.jpg" alt="System Flow Diagram" class="mx-auto d-block">
            <p>Fig<a href="#ref5">[5] </a> System flow diagram</p>
            <p><b>System flow</b><br>
              <b>Static Object Detection & Identification: </b>Smartphone Camera captures the images of indoors and sends it to server where the data from the images will be analyzed and deep learning and computer vision will be implemented.
              Then,Faster-RCNN algorithm integrates region proposal, feature extraction, classification and rectangle-refine into one end-to-end network, and detects and identifies the static object.<br>
              <b>Obtaining Control Points Coordinates: </b>Control points are those physical feature points on static objects with accurately surveyed coordinate location and can be identified relatively easyily.Below algorithm is used to obtain
            </p>
              Pixel
              Coordinates of Control Points in Test Images
              <img src="images/controlpoints.jpg" alt="System Flow Diagram" class="mx-auto d-block">
              <p> Algorithm to obtain Pixel Coordinates of Control Points in Test Images<a href="#ref6">[6] </a></p>
              <img src="images/controlpointoutput.jpg" alt="Sample output" class="mx-auto d-block">
              <p>Fig<a href="#ref6">[6] </a> An example of output of above algorithm. The pixel coordinates of control points in test image are obtained from reference image.</p>
              <b>Position Estimation: </b>The geometric relation between control points in image and object space is determined via collinear equation <br>
              <b>Distance Estimation: </b>In order to avoid gross error for the final position, distance estimation is then implemented to check the output of collinear equation model.


          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section4">
          <div class="w-100">
            <h1 class="mb-0">Success and Failures

            </h1>


            <h3>Success</h3>
            <p>
              Munich-based 3D indoor mapping and navigation startup NavVis provides computer vision based indoor mapping and navigation solution to industries.<br>
              Deep Neural network based Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) implementations have provided 70-80% success rate in real time experimental setup.<a href="#ref7">[7] </a><br>
              Apple maps used WIFI triangulation to help indoor navigation in few airports.

            </p>

            <h3>Failure</h3>
            <p>

              Indoor navigation using the sensors incur initial set up and maintenance cost. They can’t be used in large scale.<br>

              Some methods require preloading indoor maps which is not a practical solution as it’s cumbersome and not feasible to get indoor maps of every other indoors that one might visit.<br>
              Deep learning based solutions may not work in unknown places as the the objects in the area may not match the images in the trained set.


            </p>


          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section5">
          <div class="w-100">
            <h1 class="mb-0">Challenges

            </h1>

            <p>Indoor navigation with dark or Poorly lit indoors will be hard to achieve using computer vision techniques
              <br>Challenges specific to VSLAM technique:
              <br>
              1.Unknown environment-In SLAM,we don't use preloaded maps or building layouts<br>
              2.Smartphone cameras have single camera and its difficult to obtain depth data from image.<br>
              3.Complex Calculations

            </p>

          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section6">
          <div class="w-100">
            <h1 class="mb-0">Future

            </h1>

            <p>Indoor positioning and indoor navigation (IPIN) have been organizing conference providing forum for researchers in the field of indoor positioning and navigation. With plenty of papers published and with an active research community
              like
              IPIN,
              It is indeed an area where a lot of research is taking place.</p>

          </div>
        </section>

        <hr class="m-0">

        </section>
        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section7">
          <div class="w-100">
            <h1 class="mb-0">Related Videos

            </h1>

            <div class="row">

              <div class="col-sm-6">
                <iframe width="420" height="345" src="https://www.youtube.com/embed/2Y08GRYnC3U">
                </iframe>
                <p>Autonomous Indoor Flight using Deep Learning.<a href="#ref7">[7] </a> </p>
              </div>

              <div class="col-sm-6">
                <iframe width="420" height="345" src="https://youtube.com/embed/C18t5yve-Xk">
                </iframe>
                <p>Indoor navigation using computer vision tutorial</p>
              </div>
            </div>
          </div>

        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section8">
          <div class="w-100">
            <h1 class="mb-0">Quiz

            </h1>
            <p>1.Why is sensor based indoor navigation technique not preferred?</p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a. It incurs initial setup and maintenance cost</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b.High computational complexity</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c.Requires RGB-D camera</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. All of the above</label>
              </div>
            </form><br>

            <p>2.What approach uses visual odometry and RGB-D images? </p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a. WiFi triangulation </label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b. VSLAM</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c.Bluetooth beacons</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. None of the above</label>
              </div>
            </form><br>

            <p>3.Which algorithm is used for  for generating region proposals and a network using these proposals to detect objects ?</p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a.Faster-RCNN</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b.Extended Kalman filter</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c. Gaussian filter</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. Any of the above</label>
              </div>
            </form>
            <p><b>Answers:</b>
              <br>
              1.a<br>2.b<br>3.a<br>
          </div>
        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section9">
          <div class="w-100">
            <h1 class="mb-0">References

            </h1>
            <div id="ref1">
              <p>
                [1] Ruotsalainen, Laura & Kuusniemi, Heidi & Chen, Ruizhi. (2011). <a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/6071924">Heading Change Detection for Indoor Navigation with a Smartphone Camera.</a>
                International Conference on Indoor Positioning and Indoor Navigation, IPIN 2011. 21-23. 10.1109/IPIN.2011.6071924.
                <br>Synopsis: This paper presents a solution to indoor navigation problem based on vanishing points calculated from lines in consecutive images.
                <br>Reliability: Laura Ruotsalainen is currently a Research Manager and the Deputy Director of the Department of Navigation and Positioning, Finnish Geospatial Research Institute, Kirkkonummi, Finland, where she leads the Research
                Group on Sensors and Indoor Navigation. Heidi Kuusniemi is the head of the Department and a Professor with the Department of Navigation and Positioning, Finnish Geodetic Institute. Ruizhi Chen was an Endowed Chair and a Professor with
                Texas A&M University Corpus Christ, USA, and the Head and a Professor of the Department of Navigation and Positioning, Finnish Geodetic Institute, Finland.
              </p>
            </div>


            <div id="ref2">
              <p>
                [2] Han, Shibo & Ahmed, Minhaz & Rhee, Phill. (2018).<a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/8614006"> Monocular SLAM and Obstacle Removal for Indoor Navigation.</a> 67-76. 10.1109/iCMLDE.2018.00023.<br>
                Synopsis: This paper aims at removing the obstacle to enhance the SLAM system performance that based on popular open source framework ORB-SLAM2 in dynamic environment.<br>
                Reliability: Since 1992, Phill Kyu Rhee has been an Associate Professor in the Department of Computer Science and Engineering of the INHA University, Incheon, Korea and since 2001, he is a professor in the same department and
                university. This article’s work was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education
              </p>
            </div>

            <div id="ref3">
              <p>
                [3] Zhang, Xiaochen & Li, Bing & Joseph, Samleo & Xiao, Jizhong & Sun, Yi & Tian, Yingli & Munoz, J. & Yi, Chucai. (2015). <a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/7379390"> A SLAM Based Semantic Indoor
                  Navigation System for Visually Impaired Users.</a>
                1458-1463. 10.1109/SMC.2015.258.
                <br>
                Synopsis: This paper proposes a navigation system based on simultaneous localization and mapping (SLAM) and semantic path planning. This system uses multiple wearable sensors and feedback devices including a RGB-D sensor and an
                inertial measurement unit (IMU) on the waist, a head mounted camera, a microphone and an earplug/speaker.
                <br>Reliability: Jizhong Xiao is a senior member of IEEE and has published more than 160 research articles in peer reviewed journal and conferences. Yingli Tian has been a Professor with the Department of Electrical Engineering, The
                City College and the Department of Computer Science, The Graduate Center, The City University of New York, since 2008. She is one of the Inventors of the IBM Smart Surveillance Solutions.
              </p>
            </div>
            <div id="ref4">
              <p>

                [4] Sylwester Bala.<a href=" https://community.arm.com/developer/tools-software/graphics/b/blog/posts/introducing-slam-technology">“Introducing SLAM”.</a> ARM community<br>
                Synopsis:It’s a short introduction to SLAM.<br>
                Reliability:This is an article in a blog on arm community.The author is a senior software engineer at ARM.
              </p>
            </div>

            <div id="ref5">
              <p>
                [5] Taketomi, Takafumi & Uchiyama, Hideaki & Ikeda, Sei. (2017). <a href=" https://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2">Visual SLAM algorithms: a survey from 2010 to 2016.</a> IPSJ Transactions on Computer
                Vision and Applications. 9. 10.1186/s41074-017-0027-2.
                <br>Synopsis: This paper categorizes and summarizes recent vSLAM algorithms proposed in different research communities from both technical and historical points of views.
                <br>Reliability: Takafumi Taketomi is a Senior Engineer at Huawei Japan Research Center. Also, he is an Affiliate Associate Professor at the Interactive Media Design Laboratory, Graduate School of Information Science, Nara Institute
                of Science and Technology, JAPAN. Hideaki Uchiyama is an Associate Professor at National University Corporation Kyushu University


              </p>
            </div>








            <div id="ref6">
              <p>
                [6] Xiao, A., Chen, R., Li, D., Chen, Y., & Wu, D. (2018). <a href="https://www.mdpi.com/1424-8220/18/7/2229/htm">An Indoor Positioning System Based on Static Objects in Large Indoor Scenes by Using Smartphone Cameras</a> Sensors,
                18(7),
                2229. doi: 10.3390/s18072229<br>
                Synopsis: A journal article where authors propose a system to detect static objects in large indoor spaces and then calculate the smartphone’s position to locate users. The system integrates algorithms of deep learning and computer
                vision.<br>
                Reliability: The paper is supported by the National Key Research and Development Program of China, the NSFC, the State Key Laboratory Research Expenses of LIESMARS. Sensors is the leading international peer-reviewed open access
                journal on the science and technology of sensors and biosensors , published semi-monthly online by MDPI.

              </p>
            </div>




            <div id="ref7">

              <p>
                [7] Kim, Dong & Chen, Tsuhan. (2015).<a href="https://arxiv.org/pdf/1511.04668.pdf">Deep Neural Network for Real-Time Autonomous Indoor Navigation </a> arXiv:1511.04668v2 [cs.CV] 26 Nov 2015 <br>
                Synopsis: In this paper, authors propose a practical system in which a quadcopter autonomously navigates indoors and finds a specific target, i.e. a book bag, by using a single camera. A deep learning model, Convolutional Neural
                Network (ConvNet), is used to learn a controller strategy that mimics an expert pilot’s choice of action.
                <br>Reliability: Prof Chen Tsuhan is the Deputy President (Research and Technology) at National University of Singapore. He is currently the Chief Scientist of AI Singapore. Prior to his current appoint he was the Dean of the College
                of Engineering[1]at Nanyang Technological University from 2015 to 2017. He was previously Director of the School of Electrical and Computer Engineering at Cornell University from 2009 to 2013.He is also an Honorary Professor
                in Computer Science at National Chao Tung University, Taiwan. arXiv is a premier repository of electronic preprints approved for posting after moderation, but not full peer review.
              </p>
            </div>


            <h3>Further reading:</h3>

            <div id="ref8">

              <p>
                1.<a href="https://www.doc.ic.ac.uk/~ab9515/introductiontomonocular.html">Monocular Simultaneous Location and Mapping</a> <br>
                Synopsis: Information on various SLAM methods and algorithms.<br>
                Reliability: This website is by students of Department of Computing, Imperial College London. This website forms a part of a group's project for their Computing Topics course.
              </p>
            </div>




          </div>

        </section>
        <section>
        </section>


      </div>

    </div>
    <!-- /#page-content-wrapper -->

  </div>
  <!-- /#wrapper -->





</body>

</html>
