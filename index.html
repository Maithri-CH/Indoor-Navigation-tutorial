<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>

  <title>Computer vision Project 1</title>
  <style>
    body {
      position: relative;

    }

    #table_of_content {
      width: 200px;
      position: fixed;
      font-size: 15px;
      height: 100%;
      overflow: auto;
    }

    #table_of_content a:hover {
      color: #000000;
    }

    #table_of_content a {
      padding: 6px 8px 6px 6px;
      text-decoration: none;
      color: #818181;
      display: block;
    }

    div #page-content-wrapper {
      margin-left: 192px;
      font-size: 15px;
      margin-top: 50px;
      padding-top: 6px;
      position: relative;
      height: 1000px;

    }





    #section1 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section2 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section3 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section4 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section5 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section6 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section7 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    #section8 {
      padding-top: 30px;
      background-color: #dffbec;
    }

    #section9 {
      padding-top: 30px;
      background-color: #dfeefb;
    }

    h4 {
      text-align: center;
    }

    p.center {
      text-align: center;
    }
  </style>

</head>

<body data-spy="scroll" data-target="#table" data-offset="20">

  <!-- main navbar -->
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid">
      <div class="navbar-header">
        <a class="navbar-brand" href="#">Indoor Navigation Using Computer Vision</a>
      </div>
      <div>

        <ul class="nav navbar-nav">
          <!--      <li><a href="#">About</a></li> -->

        </ul>
      </div>
    </div>

  </nav>

  <!-- end of main navbar -->


  <div class="d-flex" id="wrapper">

    <!-- Sidebar -->
    <div class="bg-light border-right" id="table_of_content">


      <div class="list-group list-group-flush" id="table">

        <h4>Table of content</h4>
        <a href="#section1" class="list-group-item list-group-item-action bg-light">Introduction and Problem Definition</a>
        <a href="#section2" class="list-group-item list-group-item-action bg-light">Sensor Possibilities</a>
        <a href="#section3" class="list-group-item list-group-item-action bg-light">Indoor Navigation techniques</a>
        <a href="#section4" class="list-group-item list-group-item-action bg-light">Success and Failures</a>
        <a href="#section5" class="list-group-item list-group-item-action bg-light">Challenges</a>
        <a href="#section6" class="list-group-item list-group-item-action bg-light">Future </a>
        <a href="#section7" class="list-group-item list-group-item-action bg-light">Related Videos </a>
        <a href="#section8" class="list-group-item list-group-item-action bg-light">Quiz </a>
        <a href="#section9" class="list-group-item list-group-item-action bg-light">References </a>


      </div>
    </div>
    <!-- /#sidebar-wrapper -->

    <!-- Page Content -->
    <div id="page-content-wrapper">


      <div class="container-fluid p-0">
        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section1">
          <div class="w-100">

            <h1 class="mb-0">Introduction & Problem Definition

            </h1>

            <p> Many a time, I find it difficult to get direction indoors in a new place. I have seen many freshers at school having a hard time finding direction to their classrooms on the first day. It might get even harder for a person with low
              vision.
              <br>
              People are relying more on their smartphones for navigation.GPS works well for outdoor navigation, but it isn't accurate for indoor navigation. Currently, there is no indoor navigation system that is both economical and accurate.<br>
              The focus of this project is to find the current location of a user at indoors which will help in developing a solution to indoor navigation problem to help especially people who have low vision to navigate thru indoors.</p>

          </div>

        </section>

        <hr>


        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section2">
          <div class="w-100">
            <h1 class="mb-0">Sensor Possibilities

            </h1>

            <p>
              In this project,we will be using only the camera and sensors that are available in smartphones <br>
              There have been indoor navigation solutions using Infrared , Bluetooth beacons, wifi signal strength, RFID sensors, inertial sensors to track the user’s movements. These methods are not very economical as incur initial setup and
              maintenance cost. Also it is not practical to setup base staitions for every indoors that we need to navigate thru.

            </p>

          </div>
          <hr>
        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section3">
          <div class="w-100">
            <h1 class="mb-0">Indoor Navigation techniques based on computer vision

            </h1>
            <br>

            <h3>1.Vanishing Point Calculation</h3>
            <p>
              In this method, vanishing points are calculated from lines in consecutive images.Line features from images are selected and the vanishing points are calculated by choosing the pixel getting the maximum intersection points of line
              pairs.Some of the algorithms used in this techniques are:<br>



              <b> Gaussian filter: </b>This image processing techniques is used to reduce noise in the captured images. Once the noise reduction is done, the images will be sent for edge detection.
              <br>
              <b> Canny edge detection: </b> Edges are detected by the intensity gradient of pixels in the image. Pixels that have gradient magnitude above certain threshold are considered to make an edge.
              <br>
              <b> Hough transforms: </b>Lines that are edges has to be distinguished from the rest of the lines in the image. Hough line transform, and circle transform are used to detect lines and circles in the captured images respectively</p>
            <img src="images/corridor1.jpg" alt="corridor1" class="mx-auto d-block">
            <img src="images/corridor2.jpg" alt="corridor2" class="mx-auto d-block">

            <p class="center">Image of corridor1 on top and corridor 2 on bottom. Blue lines were found with Hough transformation and the red spot shows the place of the vanishing point </p>


            <h3>2.VSLAM</h3>
            <p>
              Visual simultaneous localization and mapping (VSLAM) technique is used for building a 3D global map of unknown environment while simultaneously keeping track of current location.

              SLAM Algorithm consists of 4 parts:<br>
              <b>1.Sensor Data: </b>For VSLAM,data is obtained mainly from camera <br>
              <b>2. Front-end: </b>Feature points are unique location in images-example: Corners,T junctions.A good feature point is one that is reliable,i.e the algorithm must find the point in different viewing conditions(different camera
              angle,onrotation,lighting change,blurring from motion etc).SLAM algorithm also provides the 3D position of the 2D feature points that the system has been tracking. This set of 3D points is known as a “point cloud”.These features also
              need to be associated with landmarks – keypoints with a 3D position, also called map points. In addition, map points need to be tracked in a video stream.The back end can provide feedback to the front end for loop closure detection and
              verification.
              Long-term association reduces drift by recognizing places that have been encountered before (loop closure).<br>
              <b>3.Back-End:</b> Here, relationship between the video frames,device orientation and geometrical reconstruction is done.<br>
              <b>4.SLAM estimate: </b>This is the reslut from the above part that contains detected features and device postion in the area.<br><br>
              <img src="images/slamOverview.jpg" alt="slam overview" class="mx-auto d-block"><br><br>
              <p class="center">SLAM Overview</p>
              <img src="images/slammethods.jpg" alt="slam methods" class="mx-auto d-block"><br>
              <p class="center">SLAM Methods</p>

              VSLAM is based on developing visual odometry from RGB-D data from the images. Hence it requires per-pixel depth sensing RGB-D camera which isn’t available in most of the smartphones today. Since depth cannot be directly inferred from a
              single image from a smartphone camera, it must be calculated through analyzing images in the chain of frames in the video using an Extended Kalman filter. Also, this method has complex computations due to which there could be
              considerable
              amount of delay in 3D reconstructions. Hence it is difficult to use VSLAM technique for Indoor navigation solution using current day smartphones.
            </p>

            <h3>3.Computer Vision + Deep learnings</h3>
            <p>
              <br>
              In this technique, deep learning and computer vision algorithms. are integrated.Smartphone camera is used to detect immovable objects like doors and windows in indoors to identify the
              current location.</p>
            <img src="images/systemflow.jpg" alt="System Flow Diagram" class="mx-auto d-block">
            <p class="center">System flow diagram</p>
            <p><b>System flow</b><br>
              <b>Static Object Detection & Identification: </b>Smartphone Camera captures the images of indoors and sends it to server where the data from the images will be analyzed and deep learning and computer vision will be implemented.
              Then,Faster-RCNN algorithm integrates region proposal, feature extraction, classification and rectangle-refine into one end-to-end network, and detects and identifies the static object.<br>
              <b>Obtaining Control Points Coordinates: </b>Control points are those physical feature points on static objects with accurately surveyed coordinate location and can be identified relatively easyily.Below algorithm is used to obtain
              Pixel
              Coordinates of Control Points in Test Images
              <img src="images/controlpoints.jpg" alt="System Flow Diagram" class="mx-auto d-block">
              <p class="center"> Algorithm to obtain Pixel Coordinates of Control Points in Test Images</p>
              <img src="images/controlpointoutput.jpg" alt="Sample output" class="mx-auto d-block">
              <p class="center"> An example of output of above algorithm. The pixel coordinates of control points in test image are obtained from reference image.</p>
              <b>Position Estimation: </b>The geometric relation between control points in image and object space is determined via collinear equation <br>
              <b>Distance Estimation: </b>In order to avoid gross error for the final position, distance estimation is then implemented to check the output of collinear equation model.
            </p>


          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section4">
          <div class="w-100">
            <h1 class="mb-0">Success and Failures

            </h1>


            <h3>Success</h3>
            <p>
              Munich-based 3D indoor mapping and navigation startup NavVis provides computer vision based indoor mapping and navigation solution to industries.<br>
              Apple maps used WIFI triangulation to help indoor navigation in few airports.<br>
              Deep Neural network based Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) implementations have provided 70-80% success rate in real time experimental setup.

            </p>

            <h3>Failure</h3>
            <p>

              Indoor navigation using the sensors incur initial set up and maintenance cost. They can’t be used in large scale.<br>

              Some methods require preloading indoor maps which is not a practical solution as it’s cumbersome and not feasible to get indoor maps of every other indoors that one might visit.<br>
              Deep learning based solutions may not work in unknown places as the the objects in the area may not match the images in the trained set.


            </p>


          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section5">
          <div class="w-100">
            <h1 class="mb-0">Challenges

            </h1>

            <p>Indoor navigation with dark or Poorly lit indoors will be hard to achieve using computer vision techniques
              <br>Challenges specific to VSLAM technique:
              <br>
              1.Unknown environment-In SLAM,we don't use preloaded maps or building layouts<br>
              2.Smartphone cameras have single camera and its difficult to obtain depth data from image.
              3.Complex Calculations

            </p>

          </div>

        </section>

        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section6">
          <div class="w-100">
            <h1 class="mb-0">Future

            </h1>

            <p>Indoor positioning and indoor navigation (IPIN) have been organizing conference providing forum for researchers in the field of indoor positioning and navigation. With plenty of papers published and with an active research community
              like
              IPIN,
              It is indeed an area where a lot of research is taking place.</p>

          </div>
        </section>

        <hr class="m-0">

        </section>
        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section7">
          <div class="w-100">
            <h1 class="mb-0">Related Videos

            </h1>

            <div class="row">

              <div class="col-sm-6">
                <iframe width="420" height="345" src="https://www.youtube.com/embed/2Y08GRYnC3U">
                </iframe>
              </div>

              <div class="col-sm-6">
                <iframe width="420" height="345" src="https://www.youtube.com/embed/WoBcsnEGiB8">
                </iframe>
              </div>
            </div>
          </div>

        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section8">
          <div class="w-100">
            <h1 class="mb-0">Quiz

            </h1>
            <p>1.Why is sensor based indoor navigation technique not preferred?</p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a. It incurs initial setup and maintenance cost</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b.High computational complexity</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c.Requires RGB-D camera</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. All of the above</label>
              </div>
            </form><br>

            <p>2.What approach uses visual odometry and RGB-D images? </p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a. WiFi triangulation </label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b. VSLAM</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c.Bluetooth beacons</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. None of the above</label>
              </div>
            </form><br>

            <p>3.Which algorithm is used for static objects detection and identification ?</p>
            <form>
              <div class="radio">
                <label><input type="radio" name="optradio">a.Faster-RCNN</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">b.Extended Kalman filter</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">c. Gaussian filter</label>
              </div>
              <div class="radio">
                <label><input type="radio" name="optradio">d. Any of the above</label>
              </div>
            </form>
            <p><b>Answers:</b>
              <br>
              1.a<br>2.b<br>3.a<br>
          </div>
        </section>


        <hr class="m-0">

        <section class="home-section p-3 p-lg-5 d-flex align-items-center" id="section9">
          <div class="w-100">
            <h1 class="mb-0">References

            </h1>

            <p><a href="https://www.mdpi.com/1424-8220/18/7/2229/htm">An Indoor Positioning System Based on Static Objects in Large Indoor Scenes by Using Smartphone Cameras</a>
              <br><a href="https://arxiv.org/pdf/1511.04668.pdf">Deep Neural Network for Real-Time Autonomous Indoor Navigation</a><br>
              <a href="https://www.researchgate.net/publication/228411072_Heading_Change_Detection_for_Indoor_Navigation_with_a_Smartphone_Camera">Heading Change Detection for Indoor Navigation with a Smartphone Camera </a></p>

            <h1>Interesting Readings </h1>
            <h3>Visual SLAM:</h3>
            <p>
              <a href="https://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2">Visual SLAM algorithms: a survey from 2010 to 2016</a><br>
              <a href="https://community.arm.com/developer/tools-software/graphics/b/blog/posts/introducing-slam-technology">Introducing SLAM</a><br>
              <a href="https://www.doc.ic.ac.uk/~ab9515/introductiontomonocular.html">Monocular Simultaneous Location and Mapping</a><br>
            </p>

            <h3>VSLAM based research papers on Indoor Navigation with RGB-D camera:</h3>
            <p>

              <a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/8614006"> Monocular SLAM and Obstacle Removal for Indoor Navigation </a><br>
              <a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/8737640">Pair-Navi: Peer-to-Peer Indoor Navigation with Mobile Visual SLAM</a><br>

              <a href="https://ieeexplore-ieee-org.proxylib.csueastbay.edu/document/7379390"> A SLAM Based Semantic Indoor Navigation System for Visually Impaired Users</a>

            </p>

          </div>

        </section>



      </div>

    </div>
    <!-- /#page-content-wrapper -->

  </div>
  <!-- /#wrapper -->





</body>

</html>
